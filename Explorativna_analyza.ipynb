{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorativna analyza\n",
    "\n",
    "## Jakub Ševcech \n",
    "\n",
    "Materialy dostupne na https://github.com/sevo/pewe-presentations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Najskor oznamenie\n",
    "\n",
    "\n",
    "# http://web.tuke.sk/fei-cit/wikt2017/challenge.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ciel prezentacie:\n",
    "zrozumitelne vysvetlit a kategorizovat rozne techniky explorativnej analyzy dat podla toho, ake data spracovavame.\n",
    "\n",
    "### Povieme si o:\n",
    "* Metrikach na zobrazenie roznych vlastnosti atributov\n",
    "* Vizualizacii dat a o tom ako ich interpretovat\n",
    "* Prejdeme si zopar statistickych testov na to aby sme zistili ci su v datach nejake vzory alebo ci existuju vyznamne rozdiely medzi vzorkami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Struktura prezentacie\n",
    "\n",
    "## Analyza po jednom (Univariate analysis)\n",
    "* Spojite atributy\n",
    "* Kategoricke atributy\n",
    "\n",
    "## Analyza po paroch atributov (Bivariate analysis)\n",
    "* Spojite - Spojite\n",
    "* Kategoricke - Spojite\n",
    "* Kategoricke - Kategoricke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn\n",
    "from sklearn import linear_model as lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.figsize'] = (5, 3)\n",
    "from IPython.display import Image, SVG, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analyza po jednom (Univariate analysis)\n",
    "\n",
    "Zobrazenie vlastnosti jedneho atributu\n",
    "\n",
    "## Spojite atributy\n",
    "\n",
    "Chceme zobrazit aky je tvar rozdelenia dat, ci sa zoskupuju okolo nejakeho **centra**, aka je **rozptylenost** hodnot\n",
    "\n",
    "### Zobrazenie centralnosti:\n",
    "* mean (priemer)\n",
    "* median (median, stredna hodnota, prostredna hodnota, centralna hodnota): hodnota, ktora rozdeluje vyssie a nizsie hodnoty\n",
    "* mode (Modus, modálna hodnota, najpravdepodobnejšia hodnota): najcastejsia hodnota (hodnota s najvacsou pravdepodobnostou vyskytu)\n",
    "\n",
    "![Comparison mean median mode](https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Comparison_mean_median_mode.svg/512px-Comparison_mean_median_mode.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rozptylenost\n",
    "\n",
    "* range (rozsah): max - min\n",
    "* quartile (kvartil): hodnota, od ktorej je 25% resp 75% hodnot vacsich \n",
    "* percentile (percentil): hodnota, od ktorej je XX% hodnot vacsich\n",
    "* inter quartile range (medzikvartilove rozpätie): rozdiel medzi 25% a 75% kvartilom, menej nachylne na outlierov ako rozsah\n",
    "![boxplot](img/outlier_box_plot.gif)\n",
    "\n",
    "* variance (variancia): priemerna kvadraticka odchylka od priemeru \n",
    "$$ E[(X-E[X])^2] $$\n",
    "* standard deviation (standardna odchylka): druha odmocnina variancie, je v jednotkach meranej premennej\n",
    "* skewness (vychylenost?): metrika symetrickosti rozdelenia, ci je rozdelenie navazene na jednu stranu \n",
    "* kurtosis (zplostenost?): ake mnozstvo dat je vo chvoste rozdelenia\n",
    "   \n",
    "(zdroj obrazku: https://taps-graph-review.wikispaces.com/Box+and+Whisker+Plots)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Skewness a Kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness\n",
    "\n",
    "Skewness je metrika toho, ako je rozdelenie symetricke. Uplne symetricke rozdelenie ma hodnotu skewness rovnu 0. V podstate to porovnava relativnu velkost dvoch chvostov rozdelenia. \n",
    "Rozdelenie naklonene do lava bude mat skewness vacsiu ako 0, naklonene doprava bude mat menej ako 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "\n",
    "norm = stats.norm(0, 1)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "sample = norm.rvs(sample_size)\n",
    "\n",
    "plt.plot(x, norm.pdf(x))\n",
    "plt.hist(sample, normed=True, bins=20)\n",
    "plt.title(\"Normalne rozdelenie: \"\"Skewness %.5f\" % (stats.skew(sample), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "chi2 = stats.chi2(5)\n",
    "x = np.linspace(0, 30, 100)\n",
    "sample = chi2.rvs(sample_size)\n",
    "\n",
    "plt.plot(x, chi2.pdf(x))\n",
    "plt.hist(sample, normed=True, bins=20)\n",
    "plt.title(\"Chi-kvadrat(5) rozdelenie: \"\"Skewness %.5f\" % (stats.skew(sample), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "chi2 = stats.chi2(5)\n",
    "x = np.linspace(0, 30, 100)\n",
    "sample = 30 - chi2.rvs(sample_size)\n",
    "\n",
    "plt.plot(x, chi2.pdf(30 - x))\n",
    "plt.hist(sample, normed=True, bins=20)\n",
    "plt.title(\"30 - Chi-kvadrat(5) rozdelenie: \"\"Skewness %.5f\" % (stats.skew(sample), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kurtosis\n",
    "\n",
    "Kurtosis hovori aka je kombinovana velkost chvostov. Meria mnozstvo dat sustredene v chvostoch. \n",
    "Velmi casto sa porovnava k hodnote kurtosis normalneho rozdelenia, ktora je 3. \n",
    "Ak je to viac ako 3, tak viac dat je sustredenych na okrajoch. Ak menej ako 3, tak je menej dat v okrajoch.\n",
    "\n",
    "Casto sa pouziva aj excess kurtosis, co je rozdiel oproti normalnemu rozdeleniu, cize kurtosis - 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100000\n",
    "\n",
    "norm = stats.norm(0, 1)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "sample = norm.rvs(sample_size)\n",
    "\n",
    "plt.plot(x, norm.pdf(x))\n",
    "plt.hist(sample, normed=True, bins=20)\n",
    "plt.title(\"Normalne rozdelenie: \"\"Kurtosis %.5f\" % (stats.kurtosis(sample), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ocakavali sme, ze dostaneme hodnotu kurtosis okolo 3. V skutocnosti funkcia stats.kurtosis pocita pri predvolenych nastaveniach excess kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100000\n",
    "\n",
    "norm = stats.norm(0, 1)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "sample = norm.rvs(sample_size)\n",
    "\n",
    "plt.plot(x, norm.pdf(x))\n",
    "plt.hist(sample, normed=True, bins=20)\n",
    "plt.title(\"Normalne rozdelenie: \"\"Kurtosis %.5f\" % (stats.kurtosis(sample, fisher=False), ))\n",
    "# musime prestavit parameter fisher na False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A teraz ukazka na nejakych rozdeleniach, kde vieme pekne kontrolovat mnozstvo dat v chvoste. Naprikl lognormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "lognorm = stats.lognorm(0.1)\n",
    "x = np.linspace(-1, 20, 100)\n",
    "sample = lognorm.rvs(sample_size)\n",
    "\n",
    "plt.plot(x, lognorm.pdf(x))\n",
    "plt.hist(sample, normed=True, bins=20)\n",
    "\n",
    "plt.title(\"LogNormalne rozdelenie (0.5): \"\"Kurtosis %.5f\" % (stats.kurtosis(sample, fisher=False), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "lognorm = stats.lognorm(1)\n",
    "x = np.linspace(-1, 20, 100)\n",
    "sample = lognorm.rvs(sample_size)\n",
    "\n",
    "plt.plot(x, lognorm.pdf(x))\n",
    "plt.hist(sample, normed=True, bins=20)\n",
    "\n",
    "plt.title(\"LogNormalne rozdelenie (1.0): \"\"Kurtosis %.5f\" % (stats.kurtosis(sample, fisher=False), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A teraz viacero rozdeleni v jednom obrazku aby sa to dalo dobre predstavit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "x = np.linspace(-5, 50, 100)\n",
    "\n",
    "dists = [\n",
    "    (\"Chi2(5)\", stats.chi2(5).pdf(x), stats.chi2(5).rvs(sample_size)),\n",
    "    (\"Chi2(10)\", stats.chi2(10).pdf(x), stats.chi2(10).rvs(sample_size)),\n",
    "    (\"Chi2(30)\", stats.chi2(30).pdf(x), stats.chi2(30).rvs(sample_size)),\n",
    "    (\"50 - Chi2(5)\", stats.chi2(5).pdf(50 - x), 50 - stats.chi2(30).rvs(sample_size)),\n",
    "    (\"Norm\", stats.norm(0, 1).pdf(x), stats.norm(0, 1).rvs(sample_size)),\n",
    "    (\"lognorm(0.5)\", stats.lognorm(0.5).pdf(x), stats.lognorm(0.5).rvs(sample_size))\n",
    "]\n",
    "\n",
    "labels = []\n",
    "\n",
    "for name, dist, sample in dists:\n",
    "    plt.plot(x, dist)\n",
    "    labels.append(\"%s - Kurt: %.3f, Skew: %.3f\" % (name, stats.kurtosis(sample, fisher=False), stats.skew(sample)))\n",
    "    \n",
    "plt.legend(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyza po jednom - spojite atributy - vizualizacia\n",
    "\n",
    "Ako ste uz urcite pochopili, tak primarne sposoby vizualizacie su histogram a box plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 100000\n",
    "\n",
    "norm = stats.norm(0, 1)\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "sample = np.concatenate([\n",
    "    stats.norm(0, 1).rvs(sample_size),\n",
    "    stats.norm(2, 0.5).rvs(sample_size),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(sample, normed=True, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc(\"lines\", markeredgewidth=0.5)\n",
    "_ = plt.boxplot(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osobne mam celkom rad spojenie boxplotu a histogramu do Violinplotu pretoze prehladne ukazuje tvar rozdelenia.\n",
    "\n",
    "Castejsie sa ale pouziva vykreslenie dvoch obrazkov (aj histogram a aj boxplot). Spolu obsahuju viac informacii ako len jeden violinplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.violinplot(sample, orient='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QQ-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "sample = stats.norm(0, 1).rvs(sample_size)\n",
    "# sample = stats.norm(10, 5).rvs(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sm.ProbPlot(sample, fit=True).qqplot(line='45')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QQ-plot je vizualna metoda na urcenie, ci dve datove sady pochadzaju z rovnakeho rozdelenia (probability plot porovnava datovu sadu s teoretickym rozdelenim).\n",
    "\n",
    "Probability plot porovnava voci zvolenemu teoretickemu rozdeleniu. V tomto pripade normalnemu.\n",
    "\n",
    "Porovnava kvantily rozdeleni.\n",
    "\n",
    "Osy su v jednotkach porovnavanych datovych sad\n",
    "\n",
    "Bod na obrazku zobrazuje hodnotu kvantilu v prvm a druhom porovnavanom datasete.\n",
    "\n",
    "Ak su datasety rovnako velke, tak je to len vykreslenie usporiadanych datasetov pomocou scatterplotu. Ak je jeden mensi, tak sa ten pouzije na urcenie kvartilov a hodnoty z druheho (vacsieho) datasetu sa interpoluju\n",
    "\n",
    "#### Na ake otazky vie odpovedat?\n",
    "\n",
    "* Pochadzaju pozorovania z rovnakeho rozdelenia?\n",
    "* Maju rozdelenia rovnaku skalu (priemer, standardnu odchylku)?\n",
    "* Je tvar porovnavanych rozdeleni podobny (rovna ciara, bez ohladu na jej posunutie a sklon)? \n",
    "* Maju rozdelenia podobne vlastnosti skewness a kurtosis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pri zakladnom nastaveni porovnava s normalnym rozdelenim. Co nam velmi nepomoze v pripade, ak nase pozorovania su z uplne ineho rozdelenia. Len nam to povie, ze je to nejake ine rozdelenie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "lognorm_sample = stats.lognorm(0.5).rvs(sample_size)\n",
    "_ = sm.ProbPlot(lognorm_sample, fit=True).qqplot(line='45')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ocividne je hovadina porovnavat tieto nase pozorovania s uplne inou distribuciou, ale moze sa to hodit, ked mame ine pozorovania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sm.ProbPlot(lognorm_sample, dist=stats.lognorm, fit=True).qqplot(line='45')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nastastie vieme zmenit teoreticke rozdelenie a mozeme sa porovnan s nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "x = stats.norm(8.25, 2.75).rvs(1000)\n",
    "y = stats.lognorm(0.5).rvs(1000)\n",
    "pp_x = sm.ProbPlot(x, fit=True)\n",
    "pp_y = sm.ProbPlot(y, fit=True)\n",
    "_ = pp_x.qqplot(line='45', other=pp_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Velmi pekny prispevok o tom ako interpretovat QQ-plot\n",
    "https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot\n",
    "\n",
    "![QQ-plot](img/qq-plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyza po jednom - kategoricke atributy\n",
    "\n",
    "Tu je najcastejsim sposobom zobrazenia frekvencna tabulka zobrazujuca bud pocty pozorovani per unikatna hodnota atrinutu alebo pomer voci celkovemu poctu pozoorvani. \n",
    "\n",
    "Graficka vizualizacia je bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds = pd.read_csv('data/diamonds.csv')\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.color.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.color.value_counts() / len(diamonds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.color.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A samozrejme sa daju pouzit aj dalsie podobne typy na zobrazenie tychto istych dat. Napriklad kolacovy graf, aj ked ten sa cita trochu tazsie ako stlpcovy graf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ak je atribut ordinalny, tak moze mat zmysel usporiadat hodnoty.\n",
    "\n",
    "//asi nema zmysel usporiadavat farby, ale tie ich pomenovania ma k tomu nabadaju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.color.value_counts()[['D', 'E', 'F', 'G', 'H', 'I', 'J']].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ked mam atribut, v ktyorom su nejake zavislosti (casova naslednost, ine poradie), tak moze mat zmysel zobrazit to pomocou ciaroveho grafu, ktory lepsie zobrazuje zmenu.\n",
    "\n",
    "Pozor, pouzivat to len v pripade ak su tam zavislosti medzi hodnotami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.color.value_counts()[['D', 'E', 'F', 'G', 'H', 'I', 'J']].plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najcastejsie sa tento graf pouziva pri casovych radoch.\n",
    "\n",
    "Napriklad sa da pouzit na zobrazenie vyvoja nejakej meranej hodnoty v case. V tomto konkretnom pripade je to obsah NO2 vo vzduchu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import airbase\n",
    "no2 = airbase.load_data()\n",
    "\n",
    "no2[\"2012-01-01 00:00\": \"2012-01-02 00:00\"].FR04012.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obcas ma dokonca zmysel prekryvat viacere ciary cez seba, na to aby ste zobrazili viacero atributov / metrik / casovych obdobi ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2[\"2012-01-01 00:00\": \"2012-01-02 00:00\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2.loc['2009':, 'FR04037'].resample('M').agg(['mean', 'median']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyza po paroch atributov (Bivariate analysis)\n",
    "\n",
    "Analyza vztahov dvoch atributov. \n",
    "\n",
    "Kedze mame spojite a kategoricke atributy, tak mozu vzniknut 3 rozne kombinacie. Pre kazdu z nich existuju metody, ktore mozeme pouzit na opisanie ich vztahov a na ich vizualizaciu.\n",
    "\n",
    "* Spojite - Spojite\n",
    "* Kategoricke - Spojite\n",
    "* Kategoricke - Kategoricke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spojity - Spojity\n",
    "\n",
    "## Scatter plot\n",
    "Najcastejsi sposob ako vizualizovat vztah dvoch spojitych atributov.\n",
    "Zobrazuje rozmiestnenie v priestore hodnot. \n",
    "\n",
    "Da sa pouzit aj na vizualizovanie skupin pozorovani. Typicky na to aby sme zistili, ci su v datach nejake prirodzene zhluky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = seaborn.load_dataset(\"iris\")\n",
    "\n",
    "plt.scatter(iris.sepal_length, iris.sepal_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ak ide o oznackovane data, tak ich vieme ofarbit pomocou znacky a pozriet sa na to, ci sa daju na zaklade tychto atributov rozdelit do skupin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in iris.groupby(\"species\"):\n",
    "    plt.scatter(group.sepal_length, group.sepal_width, label=name)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ak chcem vizualizovat vztah vsetkych kombinacii atributov, tak mozem spravit pairplot\n",
    "\n",
    "Pozor, pri velkom pocte atributov je to dost necitatelne a velmi dlho sa to vytvara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.pairplot(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samozrejme sa da aj pairplot ofarbit triedou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.pairplot(iris, hue=\"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ak by som chcel manualne vytvarat nejake pravidla na klasifikaciu, tak z tohto obrazku by som uz nejake vedel vyrobit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot sa da pouzit nie len na vizualizovanie zhlukov ale aj korelacii / zavislosti.\n",
    "\n",
    "![scatter plot correlation](img/scatter-continuous-contunious.png)\n",
    "\n",
    "Nedokaze vsak kvantifikovat silu vztahu. Na to potrebujeme nejaku inu metriku - korelaciu.\n",
    "\n",
    "Zdroj obrazku: https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/#one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Korelacia\n",
    "\n",
    "Hodnota v rozsahu [-1, 1], ktora hovori o tom, aky silny linearny vztah je medzi atributmi.\n",
    "\n",
    "* -1 perfektna negativna korelacia\n",
    "* 0 ziadna korelacia\n",
    "* 1 perfektna kladna korelacia\n",
    "\n",
    "Pearsnov korelacny koeaficient:\n",
    "$$ corr(X, Y) = \\frac{cov(X,Y)}{E[X]E[Y]} = \\frac{E[(X-E[X])(Y-E[Y])]}{E[X]E[Y]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data = pd.read_csv('data/auto-mpg.data', delim_whitespace=True, \n",
    "                       names = ['mpg', 'cylinders', 'displacement','horsepower',\n",
    "                                'weight', 'acceleration', 'model_year', 'origin', 'name'],\n",
    "                        na_values='?'\n",
    "                      )\n",
    "car_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "car_data = car_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.regplot(x=\"horsepower\", y=\"mpg\", data=car_data)\n",
    "print(\"Pearson correlation: %.3f\" % car_data.horsepower.corr(car_data.mpg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pozor** sklon regresnej ciary nehovori o sile korelacie. Len o smere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = lm.LinearRegression()\n",
    "regr.fit(car_data.horsepower.values.reshape(len(car_data),1), car_data.mpg)\n",
    "\n",
    "seaborn.regplot(x=\"horsepower\", y=\"mpg\", data=car_data)\n",
    "print(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (car_data.horsepower.corr(car_data.mpg), regr.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklon regresnej krivky je uplne iny ako velkost korelacie. Len znamienko indikujuce smer je rovnake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100)\n",
    "y = np.ones(100)\n",
    "\n",
    "synth_data = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "regr = lm.LinearRegression()\n",
    "regr.fit(synth_data.x.values.reshape(100,1), synth_data.y)\n",
    "\n",
    "seaborn.regplot(x=\"x\", y=\"y\", data=synth_data)\n",
    "print(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (synth_data.x.corr(synth_data.y), regr.coef_[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toto je extremny pripad, kde korelaciu nevieme spocitat, kedze jedna hodnota je len konstanta (nulova variancia), pricom je to pekna rovna ciara so sklonom 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100)\n",
    "y = x * (-3.14) + 3\n",
    "\n",
    "synth_data = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y\n",
    "})\n",
    "    \n",
    "regr = lm.LinearRegression()\n",
    "regr.fit(synth_data.x.values.reshape(100,1), synth_data.y)\n",
    "\n",
    "seaborn.regplot(x=\"x\", y=\"y\", data=synth_data)\n",
    "print(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (synth_data.x.corr(synth_data.y), regr.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu je perfektne linearny vztah, kde je jasne vidiet, ze ten sklon je uplne iny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100)\n",
    "y = x + stats.norm(0,1).rvs(100)\n",
    "\n",
    "synth_data = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "regr = lm.LinearRegression()\n",
    "regr.fit(synth_data.x.values.reshape(100,1), synth_data.y)\n",
    "\n",
    "seaborn.regplot(x=\"x\", y=\"y\", data=synth_data)\n",
    "print(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (synth_data.x.corr(synth_data.y), regr.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aj ked pridame trochu sumu, tak je to velmi podobne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(100)\n",
    "y = x + stats.norm(0,30).rvs(100)\n",
    "\n",
    "synth_data = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "regr = lm.LinearRegression()\n",
    "regr.fit(synth_data.x.values.reshape(100,1), synth_data.y)\n",
    "\n",
    "seaborn.regplot(x=\"x\", y=\"y\", data=synth_data)\n",
    "print(\"Pearson correlation: %.3f, Regresion coefficient: %.3f\" % (synth_data.x.corr(synth_data.y), regr.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ked ale pridame toho sumu trochu viac, tak sa nam ta korelacia zacne poriadne kazit a znova vidime, ze to s tym smerom nesuvisi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spat k datasetu o autach\n",
    "\n",
    "### Ak by sme sa chceli pozriet na korelaciu medzi vsetkymi dvojicami atributov, tak sa da pouzit takato korelacna matica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A da sa aj vykreslit pomocou teplotnej mapy aby sa nam lepsie citala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "seaborn.heatmap(car_data.corr(), ax=ax, annot=True, fmt=\".3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kategoricky - Kategoricky\n",
    "\n",
    "* Two-way table\n",
    "* Heatmap\n",
    "* Stacked bar plot\n",
    "* Chi-kvadrat testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('data/titanic/train.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frekvencna tabulka\n",
    "titanic[\"Survived\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_class = pd.crosstab(index=titanic[\"Survived\"], \n",
    "                           columns=titanic[\"Pclass\"])\n",
    "survived_class.index= [\"died\",\"survived\"]\n",
    "survived_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.heatmap(survived_class, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ak by sme chceli zobrazit percentualny podiel, tak sa da normalizovat po riadkoch, stlpcoch, alebo vsetkych datach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_class_perc = pd.crosstab(index=titanic[\"Survived\"], \n",
    "                           columns=titanic[\"Pclass\"],\n",
    "                            normalize='index') #'columns', 'all'\n",
    "survived_class_perc.index= [\"died\",\"survived\"]\n",
    "\n",
    "seaborn.heatmap(survived_class_perc, annot=True, fmt=\".4f\")\n",
    "survived_class_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daju sa tiez spravit aj tabulky s vyssimi dimenziami. Tam sa ale uz velmi rychlo straca prehladnost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=titanic[\"Survived\"], \n",
    "            columns=[titanic[\"Pclass\"], titanic[\"Sex\"]],\n",
    "            margins=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=titanic[\"Survived\"], \n",
    "            columns=[titanic[\"Pclass\"], titanic[\"Sex\"], titanic[\"Embarked\"]],\n",
    "            margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=titanic[\"Pclass\"], columns=titanic[\"Survived\"]).plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-kvadrat (Chi-squared)\n",
    "\n",
    "Tieto testy nie su zalozene na hodnotach atributov ako je to napriklad pri t-teste (kategoricka hodnota nema pre matematikov velky zmysel), ale na ich poctoch.\n",
    "\n",
    "1. Chi-kvadrat test dobrej zhody (goodness-of-fit)\n",
    "Testuje, ci rozlozenie hodnot kategorickej premennej zodpoveda ocakavanemu rozdeleniu.\n",
    "\n",
    "2. Chi-kvadrat test nezavislosti\n",
    "testuje, ci extistuje zavislost medzi dvoma kategorickymi premennymi \n",
    "\n",
    "zdroj prikladov: http://hamelg.blogspot.sk/2015/11/python-for-data-analysis-part-25-chi.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-kvadrat test dobrej zhody\n",
    "\n",
    "predstavte si, ze mame dve sady pozorovani a chceme urcit, ci su z rovnakeho rozdelenia.\n",
    "\n",
    "Konkretny priklad: demograficke udaje pre cele USA a jeden stat v spojenych statoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "national = pd.DataFrame([\"white\"]*100000 + [\"hispanic\"]*60000 +\n",
    "                        [\"black\"]*50000 + [\"asian\"]*15000 + [\"other\"]*35000)\n",
    "           \n",
    "\n",
    "minnesota = pd.DataFrame([\"white\"]*600 + [\"hispanic\"]*300 + \n",
    "                         [\"black\"]*250 +[\"asian\"]*75 + [\"other\"]*150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_table = pd.crosstab(index=national[0], columns=\"count\")\n",
    "national_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minnesota_table = pd.crosstab(index=minnesota[0], columns=\"count\")\n",
    "minnesota_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vzorec na vypocet chi-kvadrat statistiky\n",
    "$$ \\sum{\\frac{(observerd - expected)^2}{expected}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = minnesota_table\n",
    "\n",
    "national_ratios = national_table/len(national)  # pomery pre celu populaciu (referencne rozdelenie)\n",
    "print('Ratios:', national_ratios)\n",
    "\n",
    "expected = national_ratios * len(minnesota)   # ocakavane hodnoty ak by mala sledovana vzorka rovnake rozdelenie ako cela populacia\n",
    "print(\"Expected:\", expected) # ak su z rovnakeho rozdelenia, tak toto by malo byt velmi podobne ako obsah premennej minnesota_table\n",
    "\n",
    "chi_squared_stat = (((observed-expected)**2)/expected).sum()\n",
    "\n",
    "print(\"Chi-squared\", chi_squared_stat) # vysledna hodnota chi-kvadrat statistiky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nameranu statistiku musime porovnat s kritickou hodnotou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = stats.chi2.ppf(q = 0.95, # 95% confidence\n",
    "                      df = 4)   # pocet stupnov volnosti testu = pocet kategorii - 1\n",
    "\n",
    "print(\"Critical value:\", crit) # kriticka hodnota by mala byt mensia ako hodnota chi-kvadrat statistiky, ktoru sme ziskali z dat\n",
    "\n",
    "p_value = 1 - stats.chi2.cdf(x=chi_squared_stat,  # aka je p-hodnota nasho testu\n",
    "                             df=4)\n",
    "print(\"P value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kriticka hodnota je mensia ako namerana hodnota statistiky na nasej vzorke a zaroven p-hodnota je mensia ako 0.01 na zvolenej hranici istoty, takze mozeme povedat ze pozorovania su z rovnakeho rozdelenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pozor!**\n",
    "\n",
    "ak by p-hodnota nebola mensia ako 0.01, tak by sme nemohli povedat, ze mame statisticky dokaz toho, ze data su z rozneho rozdelenia. Mozeme povedat len to, ze nemame dostatok dokazov na to aby sme zamietli nulovu hypotezu a teda ze nevieme zamietnut hypotezu, ze su z rozneho rozdelenia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre pohodlnost existuje pripravena funkcia, ktora to spocita za nas\n",
    "# pozor na to, ze expected niesu namerane data pre celu populaciu ale ocakavane pocetnosti pri rovnakom pocte pozorovani ako ma \n",
    "# sledovana datova sada\n",
    "stats.chisquare(f_obs= observed, f_exp=expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-kvadrat test nezavislosti\n",
    "\n",
    "Testujeme, ci existuje zavislost medzi dvoma kategorickymi atributmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "# Sample data randomly at fixed probabilities\n",
    "voter_race = np.random.choice(a= [\"asian\",\"black\",\"hispanic\",\"other\",\"white\"],\n",
    "                              p = [0.05, 0.15 ,0.25, 0.05, 0.5],\n",
    "                              size=1000)\n",
    "\n",
    "# Sample data randomly at fixed probabilities\n",
    "voter_party = np.random.choice(a= [\"democrat\",\"independent\",\"republican\"],\n",
    "                              p = [0.4, 0.2, 0.4],\n",
    "                              size=1000)\n",
    "\n",
    "voters = pd.DataFrame({\"race\":voter_race, \n",
    "                       \"party\":voter_party})\n",
    "\n",
    "voter_tab = pd.crosstab(voters.race, voters.party, margins = True)\n",
    "voter_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = voter_tab.ix[0:5,0:3] \n",
    "observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Na zaklade sum po riadkoch a stlpcoch vyrobime ocakavane data\n",
    "expected =  np.outer(voter_tab[\"All\"][0:5],\n",
    "                     voter_tab.ix[\"All\"][0:3]) / 1000\n",
    "\n",
    "expected = pd.DataFrame(expected)\n",
    "\n",
    "expected.columns = [\"democrat\",\"independent\",\"republican\"]\n",
    "expected.index = [\"asian\",\"black\",\"hispanic\",\"other\",\"white\"]\n",
    "\n",
    "expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toto vychadza z vety o sucine nezavislich premennych:\n",
    "\n",
    "Ak A a B su nezavisle premenne take ze P(A) > 0 a P(B) > 0, tak\n",
    "$$ P(A \\cap B) = P(A) \\times P(B)$$\n",
    "\n",
    "expected obsahuje ocakavane pocetnosti v pripade ak by tato nezavislost platila. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared_stat = (((observed-expected)**2)/expected).sum().sum()\n",
    "\n",
    "print(chi_squared_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = stats.chi2.ppf(q = 0.95, # Find the critical value for 95% confidence*\n",
    "                      df = 8)   # pocet stupnov volnosti je sucin poctu kategorii pre kazdu premenny - 1. \n",
    "                                # Pocty su 3 a 5, teda 2 * 4 = 8\n",
    "\n",
    "print(\"Critical value:\", crit )\n",
    "\n",
    "p_value = 1 - stats.chi2.cdf(x=chi_squared_stat,  # Find the p-value\n",
    "                             df=8)\n",
    "print(\"P value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# znova, existuje predpripravena funkcia, ktorej stacia pozorovane pocetnosti a vrati nam \n",
    "# chi-kvadrat statistiku, p-hodnotu, pocet stupnov volnosti a ocakavane pocetnosti\n",
    "stats.chi2_contingency(observed=observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vysledok tohto testu je, ze namerana hodnota chi-kvadrat statistiky je mensia ako kriticka hodnota a nam sa nepodarilo doukazat zavislost medzi tymito dvoma premennymi.\n",
    "\n",
    "Co dava celkom zmysel, kedze sme data generovali z dvoch uplne nezavislich nahodnych premennych.\n",
    "\n",
    "Nepodaril osa nam teda vyvratit nulovu hypotezu a nemame dostatok dokazov na to, aby sme povedali, ze existuje zavislost medzi tymito dvoma premennymi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spojity - Kategoricky\n",
    "\n",
    "Tu sa najcastejsie pouziva rozdelovanie podla kategorickej hodnoty a zobrazovanie rozdeleni podmnozin numerickych hodnot napriklad pomocou histogramov alebo box-plotov.\n",
    "\n",
    "Cize viacnasobne pouzitie vizualizacii, ktore sa pouzivaju na zobrazenie spojitych atributov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no2.plot(kind='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.violinplot(no2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ak chceme overit, ci rozne podmnoziny maju rovnake/rozdielne vlastnosti, tak potrebujeme nejake statisticke testy.\n",
    "\n",
    "Najcastejsie testy su tu t-test a Anova na overenie, ci jednotlive podmnoziny maju rozne priemery. Anova sa pouziva ak porovnavame viac ako dve podmnoziny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-test\n",
    "\n",
    "Ak chceme porovnavat mnozinu s celou populaciou, tak potrebujeme jednovyberovy t-test\n",
    "\n",
    "Ak chceme porovnavat dve nezavisle mnoziny, tak potrebujeme dvojvyberovy t-test.\n",
    "\n",
    "Ak by sme chceli testovat rozdiely medzi dvoma vzorkami tej istej mnoziny, medzi ktorymi je napriklad casova zavislost (zmena hodnoty po nejakom case/ukone (vysledky testu pred a po pouziti vyucbovej metody, vaha pred a po diete ...) ), tak musime pouzit parovy t-test\n",
    "\n",
    "Teraz chceme porovnavat dve mnzoiny, tak si dame ukazku Dvojvyberoveho a Paroveho t-testu\n",
    "\n",
    "## Dvojvyberovy t-test (Two sample t-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "ages1 = stats.poisson.rvs(loc=18, mu=33, size=30)\n",
    "ages2 = stats.poisson.rvs(loc=18, mu=13, size=20)\n",
    "\n",
    "print(ages1.mean(), ages2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ages1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(a= ages1,\n",
    "                b= ages2,\n",
    "                equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-hodnota je menej ako 0.01, takze mozeme povedat, ze sme nasli statisticky vyznamny rozdiel v priemernych hodnotach tychto dvoch vzoriek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parovy t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "before= stats.norm.rvs(scale=30, loc=250, size=100)\n",
    "after = before + stats.norm.rvs(scale=5, loc=-1.25, size=100) # Upravime povodnu vzorku. \n",
    "                                                              # Simulujeme tak nejaky proces, po ktorom chceme overit zmenu\n",
    "weight_df = pd.DataFrame({\"weight_before\":before,\n",
    "                          \"weight_after\":after,\n",
    "                          \"weight_change\":after-before})\n",
    "\n",
    "weight_df.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_rel(a = before,\n",
    "                b = after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-hodnota nieje mensia ako 0.01, takze mozeme povedat, ze sme nenasli dostatok dokazov na zamietnutie nulovej hypotezy a teda nevieme povedat, ci je tam rozdiel v strednych hodnotach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA\n",
    "\n",
    "![significant](img/significant.png)\n",
    "zdroj obrazku: https://xkcd.com/882/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na zistenie rozdielov medzi viacerymi skupinami by sme nemali robit opakovany t-test len tak. Hrozi nam, ze najdeme nevyznamny vysledok len kvoli nahode. P-hodnota znamena, ze s nejakou pravdepodobnostou dosiahnuty vysledok moze byt nahoda. Casta hranica je 5% alebo 1%. Ale aj toto je pravdepodobnost, ktora obcas nastane. Ak test opakujeme viac krat, tak sa nam to moze realne stat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "races =   [\"asian\",\"black\",\"hispanic\",\"other\",\"white\"]\n",
    "\n",
    "# Generate random data\n",
    "voter_race = np.random.choice(a= races,\n",
    "                              p = [0.05, 0.15 ,0.25, 0.05, 0.5],\n",
    "                              size=1000)\n",
    "\n",
    "# Use a different distribution for white ages\n",
    "white_ages = stats.poisson.rvs(loc=18, \n",
    "                              mu=32,\n",
    "                              size=1000)\n",
    "\n",
    "voter_age = stats.poisson.rvs(loc=18,\n",
    "                              mu=30,\n",
    "                              size=1000)\n",
    "\n",
    "voter_age = np.where(voter_race==\"white\", white_ages, voter_age)\n",
    "\n",
    "# Group age data by race\n",
    "voter_frame = pd.DataFrame({\"race\":voter_race,\"age\":voter_age})\n",
    "groups = voter_frame.groupby(\"race\").groups   \n",
    "\n",
    "# Extract individual groups\n",
    "asian = voter_age[groups[\"asian\"]]\n",
    "black = voter_age[groups[\"black\"]]\n",
    "hispanic = voter_age[groups[\"hispanic\"]]\n",
    "other = voter_age[groups[\"other\"]]\n",
    "white = voter_age[groups[\"white\"]]\n",
    "\n",
    "# Perform the ANOVA\n",
    "stats.f_oneway(asian, black, hispanic, other, white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anova nam povedala, ze je tam vyznamny rozdiel medzi priemernym vekom medzi niektorymi skupinami. Nepovedala nam ale medzi ktorymi.\n",
    "\n",
    "Jeden mozny sposob ako zistit medzi ktorymi je spustit t-test nad kazdou dvojicou.\n",
    "\n",
    "Tu ale hrozi, ze oznacime nevyznamne rozdiely ako vyznamne (vid XKCD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_pairs = []\n",
    "\n",
    "for race1 in range(len(races)):\n",
    "    for race2  in range(race1+1,5):\n",
    "        race_pairs.append((races[race1], races[race2]))\n",
    "\n",
    "for race1, race2 in race_pairs: \n",
    "    print(race1, race2)\n",
    "    print(stats.ttest_ind(voter_age[groups[race1]], \n",
    "                          voter_age[groups[race2]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby sme zamedzili oznaceniu nevyznamnych rozdielov ako vyznamnych kvoli tomu, ze sme opakovali vela vyhodnoteni podmnozin, tak musime pouzit korekciu. Najcastejsie sa pouziva znizenie p-hodnoty. Ak hladame signifikanciu na urovni 5%, tak pri jednom teste musi byt p-hodnota < 0.05. Pri viacerych opakovaniach by sme mali tuto hodnotu vydelit poctom opakovani experientu. Cize nova hranica signifikancie je 0.05 / 10 = 0.005. Tato korekcia sa vola Bonferroniho korekcia.\n",
    "\n",
    "Tato korekcia ale moze byt prilis konzervativna. Namiesto nej sa pouziva Tukeyho test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "tukey = pairwise_tukeyhsd(endog=voter_age,     \n",
    "                          groups=voter_race,   \n",
    "                          alpha=0.05)          \n",
    "\n",
    "tukey.plot_simultaneous()    \n",
    "tukey.summary()              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ked sa potrebujete rozhodnut aky test pouzit, tak moze pomoct takyto rozhodovaci strom\n",
    "\n",
    "![statistical tests](img/tests.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tu je iny priklad tabulky na vyber statistickej metody \n",
    "Je to z kurzu CHS 627: Multivariate Methods in Health Statistics (z Univerzity v Alabame)\n",
    "[pdf](choosing-the-correct-statistical-test-chs-627-university-of-alabama.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDF(object):\n",
    "    def __init__(self, pdf, size=(200,200)):\n",
    "        self.pdf = pdf\n",
    "        self.size = size\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return '<iframe src={0} width={1[0]} height={1[1]}></iframe>'.format(self.pdf, self.size)\n",
    "\n",
    "    def _repr_latex_(self):\n",
    "        return r'\\includegraphics[width=1.0\\textwidth]{{{0}}}'.format(self.pdf)\n",
    "    \n",
    "PDF('choosing-the-correct-statistical-test-chs-627-university-of-alabama.pdf',size=(1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
